{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    PER, \n",
    "    NewsEmbedding,\n",
    "    Doc,\n",
    "    NewsMorphTagger,\n",
    "    NewsNERTagger,\n",
    "    NamesExtractor\n",
    ")\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import requests\n",
    "import re\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "import time\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "names_extractor = NamesExtractor(morph_vocab)\n",
    "\n",
    "session = requests.Session()\n",
    "retries = Retry(\n",
    "    total=5,              \n",
    "    backoff_factor=0.5,   \n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "session.mount(\"https://\", adapter)\n",
    "session.mount(\"http://\", adapter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = stopwords.words(\"russian\") + list(punctuation) + ['—', '«', '»', '\\'\\'']\n",
    "\n",
    "characters = [\n",
    "    \"Татарский\",\n",
    "    \"Морковин\",\n",
    "    \"Гусейн\",\n",
    "    \"Сергей\",\n",
    "    \"Лена\",\n",
    "    \"Азадовский\",\n",
    "    \"Слава\",\n",
    "    \"Зайцев\",\n",
    "    \"Вова\", \n",
    "    \"Морковина\",\n",
    "    \"Вавилен\",\n",
    "    \"Дмитрий\",\n",
    "    \"Пугин\",\n",
    "    \"Дмитрий Пугин\",\n",
    "    \"Сергей Морковин\",\n",
    "    \"Гиреев\",\n",
    "    \"Андрей\",\n",
    "    \"Андрей Гиреев\",\n",
    "    \"Леонид\",\n",
    "    \"Азадовский\",\n",
    "    \"Леонид Азадовский\",\n",
    "    \"Вовчик\",\n",
    "    \"Малой\",\n",
    "    \"Вовчик Малой\",\n",
    "    \"Саша Бло\",\n",
    "    \"Саша\",\n",
    "    \"Леша Чикунов\",\n",
    "    \"Эдик\",\n",
    "    \"Григорий\",\n",
    "    \"Владимир Ханин\",\n",
    "    \"Ханин\",\n",
    "    \"Владимир\",\n",
    "    \"Малюта\",\n",
    "    \"Аркаша\",\n",
    "    \"Алла\",\n",
    "    \"Семен\",\n",
    "    \"Фарсейкин\",\n",
    "    \"Фарсук Сейфуль-Фарсейкин\",\n",
    "    \"Фарсук\",\n",
    "    \"Фарсук Карлович\",\n",
    "    \"Манька\"]   \n",
    "\n",
    "# def cleaner(text):\n",
    "#     tokens = nltk.word_tokenize(text)\n",
    "#     tokens = [token for token in tokens if token not in noise]\n",
    "#     clean_text = \" \".join(tokens)\n",
    "#     return clean_text\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "with open(\"Pelevin_Generation_p.txt\", 'r', encoding=\"UTF-8\") as f:\n",
    "    for line in f:\n",
    "        line = re.sub(r'^\"', '', line)\n",
    "        line = re.sub(r'\"\\s*$', ' ', line)\n",
    "        line = re.sub(r'[\\t\\n\\r]+', ' ', line)\n",
    "        text += line\n",
    "\n",
    "doc = Doc(text)\n",
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocToken(stop=7, text='Leonard', pos='X', feats=<Yes>, lemma='leonard'),\n",
       " DocToken(start=8, stop=13, text='Cohen', pos='X', feats=<Yes>, lemma='cohen'),\n",
       " DocToken(start=14, stop=22, text='Когда-то', pos='ADV', feats=<Pos>, lemma='когда-то'),\n",
       " DocToken(start=23, stop=24, text='в', pos='ADP', lemma='в'),\n",
       " DocToken(start=25, stop=31, text='России', pos='PROPN', feats=<Inan,Loc,Fem,Sing>, lemma='россия')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Токенизация \n",
    "doc.segment(segmenter)\n",
    "\n",
    "#Морфологизация\n",
    "doc.tag_morph(morph_tagger)\n",
    "\n",
    "#Лемматизация\n",
    "for token in doc.tokens:\n",
    "    token.lemmatize(morph_vocab)\n",
    "\n",
    "doc.tokens[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_filter(doc):\n",
    "    filtered_spans = []\n",
    "    for span in doc.spans:\n",
    "        skip_span = False\n",
    "        for token in span.tokens:\n",
    "            lemma = token.lemma.capitalize()\n",
    "            if lemma in characters:\n",
    "                skip_span = True\n",
    "                break\n",
    "        if not skip_span:\n",
    "            filtered_spans.append(span)\n",
    "    return filtered_spans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.tag_ner(ner_tagger)\n",
    "\n",
    "\n",
    "doc.spans = char_filter(doc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получение списка личностей, организаций и локаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_per = list()\n",
    "set_per = set()\n",
    "list_loc = list()\n",
    "list_org = list()\n",
    "for span in doc.spans:\n",
    "    if (span.type == \"PER\"):\n",
    "        for token in span.tokens:\n",
    "            list_per.append(token.lemma)\n",
    "            set_per.add(token.lemma)\n",
    "            print(token.lemma)\n",
    "    elif (span.type == \"LOC\"):\n",
    "        for token in span.tokens:\n",
    "            list_loc.append(token.lemma)\n",
    "    else:\n",
    "        for token in span.tokens:\n",
    "            list_org.append(token.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_entity(entity): # Функция для фильтрации от ненужных сущностей\n",
    "    if len(entity) < 3:\n",
    "        return False\n",
    "    if re.search(r'\\d', entity):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "unique = list(set_per)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
