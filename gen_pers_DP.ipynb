{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт, предварительные штуки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Моделька"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 19:40:08.683 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/v1/ner/ner_rus_bert_torch_new.tar.gz download because of matching hashes\n",
      "c:\\Users\\ivanr\\Documents\\cod\\.venv\\Lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-03-04 19:40:42.77 WARNING in 'deeppavlov.core.models.torch_model'['torch_model'] at line 96: Unable to place component TorchTransformersSequenceTagger on GPU, since no CUDA GPUs are available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov import build_model, configs\n",
    "\n",
    "model = build_model(configs.ner.ner_rus_bert, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка и очистка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = \"\"\n",
    "with open(\"Pelevin_Generation_p.txt\", 'r', encoding=\"UTF-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, max_len=400): # Функция для разбиения текста на параграфы для быстрой обработки\n",
    "    paragraphs = text.split(\"\\n\")\n",
    "    chunks = []\n",
    "    cur_chunk = \"\"\n",
    "    for paragraph in paragraphs:\n",
    "        if len(paragraph) > max_len:\n",
    "            if cur_chunk:\n",
    "                chunks.append(cur_chunk)\n",
    "                cur_chunk = \"\"\n",
    "            for i in range(0, len(paragraph), max_len):\n",
    "                chunks.append(paragraph[i:i+max_len])\n",
    "        else:\n",
    "            if (len(cur_chunk) + len(paragraph) <= max_len):\n",
    "                cur_chunk += \" \" + paragraph if cur_chunk else paragraph\n",
    "            else:\n",
    "                if(cur_chunk):\n",
    "                    chunks.append(cur_chunk)\n",
    "                cur_chunk = paragraph\n",
    "    if cur_chunk:\n",
    "        chunks.append(cur_chunk)\n",
    "    return chunks\n",
    "\n",
    "def lemmatizer(text): # Функция для лемматизации каждой отдельной сущности\n",
    "    lemmas = m.lemmatize(text)\n",
    "    return \"\".join(lemmas).strip()\n",
    "\n",
    "def merge_entities(entities): # Функция для мерджа смежных токенов в единую сущность\n",
    "    merged = []\n",
    "    for elem in entities:\n",
    "        token, label = elem\n",
    "        if label.startswith(\"B\"):\n",
    "            merged.append([token, label[2:]])\n",
    "        else:\n",
    "            if merged:\n",
    "                merged[-1][0] += \" \" + token\n",
    "            else:\n",
    "                merged.append([token, label[2:]])\n",
    "    return merged\n",
    "\n",
    "def is_valid_entity(entity): # Функция для фильтрации от ненужных сущностей\n",
    "    if len(entity) < 3:\n",
    "        return False\n",
    "    if re.search(r'\\d', entity):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def cleaner(entity): # Функция для отсчистки сущностей от лишних символов\n",
    "    cleaned = re.sub(r\"[^A-Za-zА-Яа-яЁё\\s]\", \"\", entity)\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Извлечение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_text(text)\n",
    "res = dict()\n",
    "for chunk in chunks: # Создание массива словарей, где в каждом словаре сущности чанка -- ключ, их типы -- значение\n",
    "    chunk_fitted = model([chunk])\n",
    "    chunk_keys = tuple(chunk_fitted[0][0])\n",
    "    chunk_values = tuple(chunk_fitted[1][0])\n",
    "    res[chunk_keys] = chunk_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Причёсывание\" списка сущностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "ents0 = []\n",
    "for key in res.keys(): # Массив с локациями, организациями, персонажами \n",
    "    for i in range(len(key)):\n",
    "        if res[key][i] != 'O':\n",
    "            cleand_key = cleaner(key[i])\n",
    "            ents0.append([cleand_key, res[key][i]])\n",
    "\n",
    "ents1 = merge_entities(ents0) # Объединям разделённые на части сущности\n",
    "\n",
    "\n",
    "# Список лемматизированных сущностей\n",
    "ents_lemmatized = [[lemmatizer(ent[0]), ent[1]] for ent in ents1]\n",
    "\n",
    "ents_val_only = [ent[0] for ent in ents_lemmatized]\n",
    "freq = Counter(ents_val_only) # Создаём список особо частотных сущностей\n",
    "threshold = 5\n",
    "exclude = {entity for entity, count in freq.items() if count >= threshold}\n",
    "\n",
    "ents_clear = [ent for ent in ents_lemmatized if ent[0] not in exclude and is_valid_entity(ent[0])] # Убираем персонажей произведения и всякий мусор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Персонажи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers = []\n",
    "for ent in ents_clear:\n",
    "    if ent[1] == \"PER\" and len(ent[0]) >= 4:\n",
    "        pers.append(ent[0])\n",
    "\n",
    "pers_unique = set(pers)\n",
    "pers_unique_list = list(pers_unique)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
